digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140009698406176 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	140009698182672 [label=AddmmBackward0]
	140009698185264 -> 140009698182672
	140010236766992 [label="model.fc.bias
 (10)" fillcolor=lightblue]
	140010236766992 -> 140009698185264
	140009698185264 [label=AccumulateGrad]
	140009698185408 -> 140009698182672
	140009698185408 [label=ViewBackward0]
	140009698185360 -> 140009698185408
	140009698185360 [label=MeanBackward1]
	140009698184736 -> 140009698185360
	140009698184736 [label=ReluBackward0]
	140009698184880 -> 140009698184736
	140009698184880 [label=AddBackward0]
	140009698184208 -> 140009698184880
	140009698184208 [label=NativeBatchNormBackward0]
	140009698184688 -> 140009698184208
	140009698184688 [label=ConvolutionBackward0]
	140009698184064 -> 140009698184688
	140009698184064 [label=ReluBackward0]
	140009698183824 -> 140009698184064
	140009698183824 [label=NativeBatchNormBackward0]
	140009698185696 -> 140009698183824
	140009698185696 [label=ConvolutionBackward0]
	140009698184928 -> 140009698185696
	140009698184928 [label=ReluBackward0]
	140009698185600 -> 140009698184928
	140009698185600 [label=AddBackward0]
	140009698182864 -> 140009698185600
	140009698182864 [label=NativeBatchNormBackward0]
	140009698182816 -> 140009698182864
	140009698182816 [label=ConvolutionBackward0]
	140009698182960 -> 140009698182816
	140009698182960 [label=ReluBackward0]
	140009698183248 -> 140009698182960
	140009698183248 [label=NativeBatchNormBackward0]
	140009698183392 -> 140009698183248
	140009698183392 [label=ConvolutionBackward0]
	140009698184256 -> 140009698183392
	140009698184256 [label=ReluBackward0]
	140009698183728 -> 140009698184256
	140009698183728 [label=AddBackward0]
	140009698181760 -> 140009698183728
	140009698181760 [label=NativeBatchNormBackward0]
	140009698181520 -> 140009698181760
	140009698181520 [label=ConvolutionBackward0]
	140009698181184 -> 140009698181520
	140009698181184 [label=ReluBackward0]
	140009698115936 -> 140009698181184
	140009698115936 [label=NativeBatchNormBackward0]
	140009698116080 -> 140009698115936
	140009698116080 [label=ConvolutionBackward0]
	140009698183680 -> 140009698116080
	140009698183680 [label=ReluBackward0]
	140009698121072 -> 140009698183680
	140009698121072 [label=AddBackward0]
	140009698121216 -> 140009698121072
	140009698121216 [label=NativeBatchNormBackward0]
	140009698121504 -> 140009698121216
	140009698121504 [label=ConvolutionBackward0]
	140009698121840 -> 140009698121504
	140009698121840 [label=ReluBackward0]
	140009698123712 -> 140009698121840
	140009698123712 [label=NativeBatchNormBackward0]
	140009698123904 -> 140009698123712
	140009698123904 [label=ConvolutionBackward0]
	140009698124240 -> 140009698123904
	140009698124240 [label=ReluBackward0]
	140009698124480 -> 140009698124240
	140009698124480 [label=AddBackward0]
	140009698122032 -> 140009698124480
	140009698122032 [label=NativeBatchNormBackward0]
	140009698125920 -> 140009698122032
	140009698125920 [label=ConvolutionBackward0]
	140009698126688 -> 140009698125920
	140009698126688 [label=ReluBackward0]
	140009698126976 -> 140009698126688
	140009698126976 [label=NativeBatchNormBackward0]
	140009698127120 -> 140009698126976
	140009698127120 [label=ConvolutionBackward0]
	140009698124576 -> 140009698127120
	140009698124576 [label=ReluBackward0]
	140009698126256 -> 140009698124576
	140009698126256 [label=AddBackward0]
	140009698125440 -> 140009698126256
	140009698125440 [label=NativeBatchNormBackward0]
	140009698125296 -> 140009698125440
	140009698125296 [label=ConvolutionBackward0]
	140009698123040 -> 140009698125296
	140009698123040 [label=ReluBackward0]
	140009698127504 -> 140009698123040
	140009698127504 [label=NativeBatchNormBackward0]
	140009698122224 -> 140009698127504
	140009698122224 [label=ConvolutionBackward0]
	140009698128944 -> 140009698122224
	140009698128944 [label=ReluBackward0]
	140009698129184 -> 140009698128944
	140009698129184 [label=AddBackward0]
	140009698131152 -> 140009698129184
	140009698131152 [label=NativeBatchNormBackward0]
	140009698128176 -> 140009698131152
	140009698128176 [label=ConvolutionBackward0]
	140009698116704 -> 140009698128176
	140009698116704 [label=ReluBackward0]
	140009698119488 -> 140009698116704
	140009698119488 [label=NativeBatchNormBackward0]
	140009698118720 -> 140009698119488
	140009698118720 [label=ConvolutionBackward0]
	140009698129280 -> 140009698118720
	140009698129280 [label=ReluBackward0]
	140009698128512 -> 140009698129280
	140009698128512 [label=AddBackward0]
	140009698120736 -> 140009698128512
	140009698120736 [label=NativeBatchNormBackward0]
	140009698120496 -> 140009698120736
	140009698120496 [label=ConvolutionBackward0]
	140009698130528 -> 140009698120496
	140009698130528 [label=ReluBackward0]
	140009698118432 -> 140009698130528
	140009698118432 [label=NativeBatchNormBackward0]
	140009698131584 -> 140009698118432
	140009698131584 [label=ConvolutionBackward0]
	140009698119728 -> 140009698131584
	140009698119728 [label=MaxPool2DWithIndicesBackward0]
	140009698130240 -> 140009698119728
	140009698130240 [label=ReluBackward0]
	140009698128368 -> 140009698130240
	140009698128368 [label=NativeBatchNormBackward0]
	140009698130096 -> 140009698128368
	140009698130096 [label=ConvolutionBackward0]
	140009698117184 -> 140009698130096
	140009696754032 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140009696754032 -> 140009698117184
	140009698117184 [label=AccumulateGrad]
	140009698117232 -> 140009698128368
	140009696372720 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	140009696372720 -> 140009698117232
	140009698117232 [label=AccumulateGrad]
	140009698118192 -> 140009698128368
	140010236310480 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	140010236310480 -> 140009698118192
	140009698118192 [label=AccumulateGrad]
	140009698116992 -> 140009698131584
	140010234234976 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140010234234976 -> 140009698116992
	140009698116992 [label=AccumulateGrad]
	140009698131728 -> 140009698118432
	140009700584608 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140009700584608 -> 140009698131728
	140009698131728 [label=AccumulateGrad]
	140009698131008 -> 140009698118432
	140009699765168 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140009699765168 -> 140009698131008
	140009698131008 [label=AccumulateGrad]
	140009698128224 -> 140009698120496
	140009696220384 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140009696220384 -> 140009698128224
	140009698128224 [label=AccumulateGrad]
	140009698130336 -> 140009698120736
	140009696228384 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140009696228384 -> 140009698130336
	140009698130336 [label=AccumulateGrad]
	140009698128560 -> 140009698120736
	140009696739632 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140009696739632 -> 140009698128560
	140009698128560 [label=AccumulateGrad]
	140009698119728 -> 140009698128512
	140009698119344 -> 140009698118720
	140009696754912 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140009696754912 -> 140009698119344
	140009698119344 [label=AccumulateGrad]
	140009698117472 -> 140009698119488
	140009696754832 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140009696754832 -> 140009698117472
	140009698117472 [label=AccumulateGrad]
	140009698131344 -> 140009698119488
	140009696754992 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140009696754992 -> 140009698131344
	140009698131344 [label=AccumulateGrad]
	140009698130288 -> 140009698128176
	140009696752432 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140009696752432 -> 140009698130288
	140009698130288 [label=AccumulateGrad]
	140009698131296 -> 140009698131152
	140009696752512 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140009696752512 -> 140009698131296
	140009698131296 [label=AccumulateGrad]
	140009698131104 -> 140009698131152
	140009696752592 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140009696752592 -> 140009698131104
	140009698131104 [label=AccumulateGrad]
	140009698129280 -> 140009698129184
	140009698128848 -> 140009698122224
	140009696854960 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140009696854960 -> 140009698128848
	140009698128848 [label=AccumulateGrad]
	140009698127408 -> 140009698127504
	140009696854800 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140009696854800 -> 140009698127408
	140009698127408 [label=AccumulateGrad]
	140009698123376 -> 140009698127504
	140009696855040 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140009696855040 -> 140009698123376
	140009698123376 [label=AccumulateGrad]
	140009698123520 -> 140009698125296
	140009696855520 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140009696855520 -> 140009698123520
	140009698123520 [label=AccumulateGrad]
	140009698125200 -> 140009698125440
	140009696855600 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140009696855600 -> 140009698125200
	140009698125200 [label=AccumulateGrad]
	140009698125536 -> 140009698125440
	140009696855680 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140009696855680 -> 140009698125536
	140009698125536 [label=AccumulateGrad]
	140009698126064 -> 140009698126256
	140009698126064 [label=NativeBatchNormBackward0]
	140009698128752 -> 140009698126064
	140009698128752 [label=ConvolutionBackward0]
	140009698128944 -> 140009698128752
	140009698128992 -> 140009698128752
	140009696755072 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140009696755072 -> 140009698128992
	140009698128992 [label=AccumulateGrad]
	140009698123232 -> 140009698126064
	140009696755232 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140009696755232 -> 140009698123232
	140009698123232 [label=AccumulateGrad]
	140009698125008 -> 140009698126064
	140009696755152 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140009696755152 -> 140009698125008
	140009698125008 [label=AccumulateGrad]
	140009698124672 -> 140009698127120
	140009696856240 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140009696856240 -> 140009698124672
	140009698124672 [label=AccumulateGrad]
	140009698127072 -> 140009698126976
	140009696856160 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140009696856160 -> 140009698127072
	140009698127072 [label=AccumulateGrad]
	140009698126784 -> 140009698126976
	140009696856320 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140009696856320 -> 140009698126784
	140009698126784 [label=AccumulateGrad]
	140009698126640 -> 140009698125920
	140009696856800 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140009696856800 -> 140009698126640
	140009698126640 [label=AccumulateGrad]
	140009698124816 -> 140009698122032
	140009696856880 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140009696856880 -> 140009698124816
	140009698124816 [label=AccumulateGrad]
	140009698121936 -> 140009698122032
	140009696856960 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140009696856960 -> 140009698121936
	140009698121936 [label=AccumulateGrad]
	140009698124576 -> 140009698124480
	140009698124144 -> 140009698123904
	140009696858080 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140009696858080 -> 140009698124144
	140009698124144 [label=AccumulateGrad]
	140009698123808 -> 140009698123712
	140009696857680 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140009696857680 -> 140009698123808
	140009698123808 [label=AccumulateGrad]
	140009698122080 -> 140009698123712
	140009696858000 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140009696858000 -> 140009698122080
	140009698122080 [label=AccumulateGrad]
	140009698121744 -> 140009698121504
	140009696858560 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140009696858560 -> 140009698121744
	140009698121744 [label=AccumulateGrad]
	140009698121408 -> 140009698121216
	140009696858480 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140009696858480 -> 140009698121408
	140009698121408 [label=AccumulateGrad]
	140009698121312 -> 140009698121216
	140009696858640 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140009696858640 -> 140009698121312
	140009698121312 [label=AccumulateGrad]
	140009698121168 -> 140009698121072
	140009698121168 [label=NativeBatchNormBackward0]
	140009698124048 -> 140009698121168
	140009698124048 [label=ConvolutionBackward0]
	140009698124240 -> 140009698124048
	140009698124336 -> 140009698124048
	140009696857440 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140009696857440 -> 140009698124336
	140009698124336 [label=AccumulateGrad]
	140009698121648 -> 140009698121168
	140009696857360 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140009696857360 -> 140009698121648
	140009698121648 [label=AccumulateGrad]
	140009698121600 -> 140009698121168
	140009696857520 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140009696857520 -> 140009698121600
	140009698121600 [label=AccumulateGrad]
	140009698120448 -> 140009698116080
	140010231374176 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140010231374176 -> 140009698120448
	140009698120448 [label=AccumulateGrad]
	140009698116032 -> 140009698115936
	140010232101712 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140010232101712 -> 140009698116032
	140009698116032 [label=AccumulateGrad]
	140009698115744 -> 140009698115936
	140010768427392 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140010768427392 -> 140009698115744
	140009698115744 [label=AccumulateGrad]
	140009698181280 -> 140009698181520
	140009696743552 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140009696743552 -> 140009698181280
	140009698181280 [label=AccumulateGrad]
	140009698181616 -> 140009698181760
	140009696744992 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140009696744992 -> 140009698181616
	140009698181616 [label=AccumulateGrad]
	140009698181712 -> 140009698181760
	140009696744752 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140009696744752 -> 140009698181712
	140009698181712 [label=AccumulateGrad]
	140009698183680 -> 140009698183728
	140009698183440 -> 140009698183392
	140009696741632 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140009696741632 -> 140009698183440
	140009698183440 [label=AccumulateGrad]
	140009698183200 -> 140009698183248
	140009696744112 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140009696744112 -> 140009698183200
	140009698183200 [label=AccumulateGrad]
	140009698183152 -> 140009698183248
	140009696740832 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140009696740832 -> 140009698183152
	140009698183152 [label=AccumulateGrad]
	140009698183008 -> 140009698182816
	140010230552896 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140010230552896 -> 140009698183008
	140009698183008 [label=AccumulateGrad]
	140009698182576 -> 140009698182864
	140009698406256 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140009698406256 -> 140009698182576
	140009698182576 [label=AccumulateGrad]
	140009698182384 -> 140009698182864
	140009698394736 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140009698394736 -> 140009698182384
	140009698182384 [label=AccumulateGrad]
	140009698192512 -> 140009698185600
	140009698192512 [label=NativeBatchNormBackward0]
	140009698183488 -> 140009698192512
	140009698183488 [label=ConvolutionBackward0]
	140009698184256 -> 140009698183488
	140009698183632 -> 140009698183488
	140009696739392 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140009696739392 -> 140009698183632
	140009698183632 [label=AccumulateGrad]
	140009698182768 -> 140009698192512
	140009696739472 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140009696739472 -> 140009698182768
	140009698182768 [label=AccumulateGrad]
	140009698182912 -> 140009698192512
	140009696740272 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140009696740272 -> 140009698182912
	140009698182912 [label=AccumulateGrad]
	140009698194624 -> 140009698185696
	140009696223744 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140009696223744 -> 140009698194624
	140009698194624 [label=AccumulateGrad]
	140009698184496 -> 140009698183824
	140009696229984 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140009696229984 -> 140009698184496
	140009698184496 [label=AccumulateGrad]
	140009698186032 -> 140009698183824
	140009696228464 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140009696228464 -> 140009698186032
	140009698186032 [label=AccumulateGrad]
	140009698184160 -> 140009698184688
	140009696224064 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140009696224064 -> 140009698184160
	140009698184160 [label=AccumulateGrad]
	140009698184352 -> 140009698184208
	140009696220624 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140009696220624 -> 140009698184352
	140009698184352 [label=AccumulateGrad]
	140009698184784 -> 140009698184208
	140010236035856 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140010236035856 -> 140009698184784
	140009698184784 [label=AccumulateGrad]
	140009698184928 -> 140009698184880
	140009698184976 -> 140009698182672
	140009698184976 [label=TBackward0]
	140009698185024 -> 140009698184976
	140009700584688 [label="model.fc.weight
 (10, 512)" fillcolor=lightblue]
	140009700584688 -> 140009698185024
	140009698185024 [label=AccumulateGrad]
	140009698182672 -> 140009698406176
}
