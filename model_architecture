digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140195055595616 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	140195586036576 [label=AddmmBackward0]
	140195586040464 -> 140195586036576
	140195052688688 [label="model.fc.bias
 (10)" fillcolor=lightblue]
	140195052688688 -> 140195586040464
	140195586040464 [label=AccumulateGrad]
	140195586040224 -> 140195586036576
	140195586040224 [label=ViewBackward0]
	140195586040128 -> 140195586040224
	140195586040128 [label=MeanBackward1]
	140195586041232 -> 140195586040128
	140195586041232 [label=ReluBackward0]
	140195586041184 -> 140195586041232
	140195586041184 [label=AddBackward0]
	140195586041472 -> 140195586041184
	140195586041472 [label=NativeBatchNormBackward0]
	140195586041328 -> 140195586041472
	140195586041328 [label=ConvolutionBackward0]
	140195586042096 -> 140195586041328
	140195586042096 [label=ReluBackward0]
	140195586041712 -> 140195586042096
	140195586041712 [label=NativeBatchNormBackward0]
	140195586035616 -> 140195586041712
	140195586035616 [label=ConvolutionBackward0]
	140195586040896 -> 140195586035616
	140195586040896 [label=ReluBackward0]
	140195586034752 -> 140195586040896
	140195586034752 [label=AddBackward0]
	140195586036048 -> 140195586034752
	140195586036048 [label=NativeBatchNormBackward0]
	140195586047808 -> 140195586036048
	140195586047808 [label=ConvolutionBackward0]
	140195586047664 -> 140195586047808
	140195586047664 [label=ReluBackward0]
	140195586044448 -> 140195586047664
	140195586044448 [label=NativeBatchNormBackward0]
	140195586044400 -> 140195586044448
	140195586044400 [label=ConvolutionBackward0]
	140195586046032 -> 140195586044400
	140195586046032 [label=ReluBackward0]
	140195586045312 -> 140195586046032
	140195586045312 [label=AddBackward0]
	140195586045456 -> 140195586045312
	140195586045456 [label=NativeBatchNormBackward0]
	140195586045072 -> 140195586045456
	140195586045072 [label=ConvolutionBackward0]
	140195586047424 -> 140195586045072
	140195586047424 [label=ReluBackward0]
	140195586047328 -> 140195586047424
	140195586047328 [label=NativeBatchNormBackward0]
	140195586047184 -> 140195586047328
	140195586047184 [label=ConvolutionBackward0]
	140195586045600 -> 140195586047184
	140195586045600 [label=ReluBackward0]
	140195586044544 -> 140195586045600
	140195586044544 [label=AddBackward0]
	140195586047040 -> 140195586044544
	140195586047040 [label=NativeBatchNormBackward0]
	140195586046944 -> 140195586047040
	140195586046944 [label=ConvolutionBackward0]
	140195586037056 -> 140195586046944
	140195586037056 [label=ReluBackward0]
	140195586039792 -> 140195586037056
	140195586039792 [label=NativeBatchNormBackward0]
	140195586039744 -> 140195586039792
	140195586039744 [label=ConvolutionBackward0]
	140195586037584 -> 140195586039744
	140195586037584 [label=ReluBackward0]
	140195586039600 -> 140195586037584
	140195586039600 [label=AddBackward0]
	140195586042144 -> 140195586039600
	140195586042144 [label=NativeBatchNormBackward0]
	140195586045024 -> 140195586042144
	140195586045024 [label=ConvolutionBackward0]
	140195586046464 -> 140195586045024
	140195586046464 [label=ReluBackward0]
	140195586038832 -> 140195586046464
	140195586038832 [label=NativeBatchNormBackward0]
	140195586038784 -> 140195586038832
	140195586038784 [label=ConvolutionBackward0]
	140195586039360 -> 140195586038784
	140195586039360 [label=ReluBackward0]
	140195586046176 -> 140195586039360
	140195586046176 [label=AddBackward0]
	140195586046080 -> 140195586046176
	140195586046080 [label=NativeBatchNormBackward0]
	140195586038352 -> 140195586046080
	140195586038352 [label=ConvolutionBackward0]
	140195586037920 -> 140195586038352
	140195586037920 [label=ReluBackward0]
	140195586038256 -> 140195586037920
	140195586038256 [label=NativeBatchNormBackward0]
	140195586037680 -> 140195586038256
	140195586037680 [label=ConvolutionBackward0]
	140195586046656 -> 140195586037680
	140195586046656 [label=ReluBackward0]
	140195586046560 -> 140195586046656
	140195586046560 [label=AddBackward0]
	140195586040320 -> 140195586046560
	140195586040320 [label=NativeBatchNormBackward0]
	140195586037488 -> 140195586040320
	140195586037488 [label=ConvolutionBackward0]
	140195586040608 -> 140195586037488
	140195586040608 [label=ReluBackward0]
	140195586041952 -> 140195586040608
	140195586041952 [label=NativeBatchNormBackward0]
	140195586041904 -> 140195586041952
	140195586041904 [label=ConvolutionBackward0]
	140195586040368 -> 140195586041904
	140195586040368 [label=ReluBackward0]
	140195586034704 -> 140195586040368
	140195586034704 [label=AddBackward0]
	140195586035040 -> 140195586034704
	140195586035040 [label=NativeBatchNormBackward0]
	140195586035568 -> 140195586035040
	140195586035568 [label=ConvolutionBackward0]
	140195586035424 -> 140195586035568
	140195586035424 [label=ReluBackward0]
	140195586034368 -> 140195586035424
	140195586034368 [label=NativeBatchNormBackward0]
	140195586034992 -> 140195586034368
	140195586034992 [label=ConvolutionBackward0]
	140195586035712 -> 140195586034992
	140195586035712 [label=MaxPool2DWithIndicesBackward0]
	140195586034800 -> 140195586035712
	140195586034800 [label=ReluBackward0]
	140195586034224 -> 140195586034800
	140195586034224 [label=NativeBatchNormBackward0]
	140195586044304 -> 140195586034224
	140195586044304 [label=ConvolutionBackward0]
	140195586044112 -> 140195586044304
	140198385505504 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140198385505504 -> 140195586044112
	140195586044112 [label=AccumulateGrad]
	140195586044352 -> 140195586034224
	140198385505424 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	140198385505424 -> 140195586044352
	140195586044352 [label=AccumulateGrad]
	140195586035472 -> 140195586034224
	140198385505344 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	140198385505344 -> 140195586035472
	140195586035472 [label=AccumulateGrad]
	140195586034272 -> 140195586034992
	140195054936416 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140195054936416 -> 140195586034272
	140195586034272 [label=AccumulateGrad]
	140195586034416 -> 140195586034368
	140195589015328 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140195589015328 -> 140195586034416
	140195586034416 [label=AccumulateGrad]
	140195586035184 -> 140195586034368
	140198385503344 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140198385503344 -> 140195586035184
	140195586035184 [label=AccumulateGrad]
	140195586036528 -> 140195586035568
	140198385506464 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140198385506464 -> 140195586036528
	140195586036528 [label=AccumulateGrad]
	140195586034512 -> 140195586035040
	140198385506304 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140198385506304 -> 140195586034512
	140195586034512 [label=AccumulateGrad]
	140195586036672 -> 140195586035040
	140198385506384 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140198385506384 -> 140195586036672
	140195586036672 [label=AccumulateGrad]
	140195586035712 -> 140195586034704
	140195586034608 -> 140195586041904
	140198385507424 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140198385507424 -> 140195586034608
	140195586034608 [label=AccumulateGrad]
	140195586042000 -> 140195586041952
	140198385507344 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140198385507344 -> 140195586042000
	140195586042000 [label=AccumulateGrad]
	140195586040560 -> 140195586041952
	140198385507264 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140198385507264 -> 140195586040560
	140195586040560 [label=AccumulateGrad]
	140195586037152 -> 140195586037488
	140198385507504 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140198385507504 -> 140195586037152
	140195586037152 [label=AccumulateGrad]
	140195586037440 -> 140195586040320
	140198385508064 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140198385508064 -> 140195586037440
	140195586037440 [label=AccumulateGrad]
	140195586040272 -> 140195586040320
	140198385507984 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140198385507984 -> 140195586040272
	140195586040272 [label=AccumulateGrad]
	140195586040368 -> 140195586046560
	140195586046704 -> 140195586037680
	140198385506144 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140198385506144 -> 140195586046704
	140195586046704 [label=AccumulateGrad]
	140195586037104 -> 140195586038256
	140198385506064 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140198385506064 -> 140195586037104
	140195586037104 [label=AccumulateGrad]
	140195586038208 -> 140195586038256
	140198385506224 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140198385506224 -> 140195586038208
	140195586038208 [label=AccumulateGrad]
	140195586038496 -> 140195586038352
	140198385509504 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140198385509504 -> 140195586038496
	140195586038496 [label=AccumulateGrad]
	140195586038016 -> 140195586046080
	140198385509664 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140198385509664 -> 140195586038016
	140195586038016 [label=AccumulateGrad]
	140195586037008 -> 140195586046080
	140198385509584 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140198385509584 -> 140195586037008
	140195586037008 [label=AccumulateGrad]
	140195586046128 -> 140195586046176
	140195586046128 [label=NativeBatchNormBackward0]
	140195586037728 -> 140195586046128
	140195586037728 [label=ConvolutionBackward0]
	140195586046656 -> 140195586037728
	140195586046608 -> 140195586037728
	140198385508544 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140198385508544 -> 140195586046608
	140195586046608 [label=AccumulateGrad]
	140195586038640 -> 140195586046128
	140195053222576 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140195053222576 -> 140195586038640
	140195586038640 [label=AccumulateGrad]
	140195586038160 -> 140195586046128
	140198385508624 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140198385508624 -> 140195586038160
	140195586038160 [label=AccumulateGrad]
	140195586046224 -> 140195586038784
	140198385509744 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140198385509744 -> 140195586046224
	140195586046224 [label=AccumulateGrad]
	140195586038880 -> 140195586038832
	140198385509904 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140198385509904 -> 140195586038880
	140195586038880 [label=AccumulateGrad]
	140195586039072 -> 140195586038832
	140198385510384 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140198385510384 -> 140195586039072
	140195586039072 [label=AccumulateGrad]
	140195586044928 -> 140195586045024
	140198385510624 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140198385510624 -> 140195586044928
	140195586044928 [label=AccumulateGrad]
	140195586044736 -> 140195586042144
	140198385510544 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140198385510544 -> 140195586044736
	140195586044736 [label=AccumulateGrad]
	140195586044784 -> 140195586042144
	140198385511184 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140198385511184 -> 140195586044784
	140195586044784 [label=AccumulateGrad]
	140195586039360 -> 140195586039600
	140195586039696 -> 140195586039744
	140198385512224 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140198385512224 -> 140195586039696
	140195586039696 [label=AccumulateGrad]
	140195586039984 -> 140195586039792
	140198385512304 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140198385512304 -> 140195586039984
	140195586039984 [label=AccumulateGrad]
	140195586039648 -> 140195586039792
	140198385512144 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140198385512144 -> 140195586039648
	140195586039648 [label=AccumulateGrad]
	140195586046752 -> 140195586046944
	140198385511824 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140198385511824 -> 140195586046752
	140195586046752 [label=AccumulateGrad]
	140195586046896 -> 140195586047040
	140198385511744 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140198385511744 -> 140195586046896
	140195586046896 [label=AccumulateGrad]
	140195586046992 -> 140195586047040
	140198385511584 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140198385511584 -> 140195586046992
	140195586046992 [label=AccumulateGrad]
	140195586047088 -> 140195586044544
	140195586047088 [label=NativeBatchNormBackward0]
	140195586039840 -> 140195586047088
	140195586039840 [label=ConvolutionBackward0]
	140195586037584 -> 140195586039840
	140195586038736 -> 140195586039840
	140198385508704 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140198385508704 -> 140195586038736
	140195586038736 [label=AccumulateGrad]
	140195586046800 -> 140195586047088
	140198385510864 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140198385510864 -> 140195586046800
	140195586046800 [label=AccumulateGrad]
	140195586046848 -> 140195586047088
	140198385508784 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140198385508784 -> 140195586046848
	140195586046848 [label=AccumulateGrad]
	140195586044592 -> 140195586047184
	140198385513424 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140198385513424 -> 140195586044592
	140195586044592 [label=AccumulateGrad]
	140195586047232 -> 140195586047328
	140198385513504 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140198385513504 -> 140195586047232
	140195586047232 [label=AccumulateGrad]
	140195586047376 -> 140195586047328
	140198385513584 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140198385513584 -> 140195586047376
	140195586047376 [label=AccumulateGrad]
	140195586047472 -> 140195586045072
	140198385512784 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140198385512784 -> 140195586047472
	140195586047472 [label=AccumulateGrad]
	140195586044496 -> 140195586045456
	140198385512624 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140198385512624 -> 140195586044496
	140195586044496 [label=AccumulateGrad]
	140195586045648 -> 140195586045456
	140198385513824 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140198385513824 -> 140195586045648
	140195586045648 [label=AccumulateGrad]
	140195586045600 -> 140195586045312
	140195586045552 -> 140195586044400
	140198385515504 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140198385515504 -> 140195586045552
	140195586045552 [label=AccumulateGrad]
	140195586033168 -> 140195586044448
	140198385515424 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140198385515424 -> 140195586033168
	140195586033168 [label=AccumulateGrad]
	140195586047568 -> 140195586044448
	140198385515584 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140198385515584 -> 140195586047568
	140195586047568 [label=AccumulateGrad]
	140195586047616 -> 140195586047808
	140198385516064 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140198385516064 -> 140195586047616
	140195586047616 [label=AccumulateGrad]
	140195586034464 -> 140195586036048
	140198385516144 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140198385516144 -> 140195586034464
	140195586034464 [label=AccumulateGrad]
	140195586034656 -> 140195586036048
	140198385516224 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140198385516224 -> 140195586034656
	140195586034656 [label=AccumulateGrad]
	140195586035520 -> 140195586034752
	140195586035520 [label=NativeBatchNormBackward0]
	140195586045744 -> 140195586035520
	140195586045744 [label=ConvolutionBackward0]
	140195586046032 -> 140195586045744
	140195586045888 -> 140195586045744
	140198385514864 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140198385514864 -> 140195586045888
	140195586045888 [label=AccumulateGrad]
	140195586047712 -> 140195586035520
	140198385514544 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140198385514544 -> 140195586047712
	140195586047712 [label=AccumulateGrad]
	140195586047760 -> 140195586035520
	140198385514944 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140198385514944 -> 140195586047760
	140195586047760 [label=AccumulateGrad]
	140195586036720 -> 140195586035616
	140198385762528 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140198385762528 -> 140195586036720
	140195586036720 [label=AccumulateGrad]
	140195586042864 -> 140195586041712
	140198385762448 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140198385762448 -> 140195586042864
	140195586042864 [label=AccumulateGrad]
	140195586042336 -> 140195586041712
	140198385762608 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140198385762608 -> 140195586042336
	140195586042336 [label=AccumulateGrad]
	140195586041808 -> 140195586041328
	140198385763088 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140198385763088 -> 140195586041808
	140195586041808 [label=AccumulateGrad]
	140195586041136 -> 140195586041472
	140198385763168 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140198385763168 -> 140195586041136
	140195586041136 [label=AccumulateGrad]
	140195586041616 -> 140195586041472
	140198385763248 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140198385763248 -> 140195586041616
	140195586041616 [label=AccumulateGrad]
	140195586040896 -> 140195586041184
	140195586040176 -> 140195586036576
	140195586040176 [label=TBackward0]
	140195586041040 -> 140195586040176
	140195052688768 [label="model.fc.weight
 (10, 512)" fillcolor=lightblue]
	140195052688768 -> 140195586041040
	140195586041040 [label=AccumulateGrad]
	140195586036576 -> 140195055595616
}
